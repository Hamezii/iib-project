N_a=100, N_b=100, f=0.4:
000: with 1 impulse, got to 0.01 loss after 35 steps
001: with 2 impulses, wasn't able to solve after 200 steps, 0.09 error for each 0 target
002: with 2 impulses, same issue but reversed! Didn't get plot for 1 output but same error

C Sparsity 0.1 > 0.2, C std 1 > 0.5
003: Didn't learn properly

004: 1 length, global inhibitory, no positive constraint, P=16, f=0.1
005: ", P=2, f=0.4
006: Test with length 2, Na=40, Nb=100. Added scheduler. Only inhibitory on input neurons.
007: Na=100, Nb=100, training steps=400: Near zero error! But almost identical output?
Try with length 4:
At step 100 levelling out to 0.69 error, i.e. balanced output
Switching to Na=100, Nb=1000:
Bouncing between ~2 to ~60 error for first 100 steps.
Stagnates 3-10 error.
Maybe learning rate needs to be scaled with neuron count?
008: Now trying with length 3, still didn't get good results, around 10 error
009: Trying 0.5 second duration len 2: it works
010 (folder): Trying on SSH, length 3, N_b = 200.
100 (folder): Trying sequence memory task. I_b=8.0. These are tests, after 1 time step. 
Random input into computational neuron and separation of input types for sequence and test not yet implemented.
101 (folder): Trying with I_b=1.5.
102 (folder): Saving loss plot and state dict.
103 (folder): Use learning rate 2e-4, with fixed lr scheduling.
104 (folder): Used much lower learning rate 1e-5. Stopped at step 10 with error 16.8. Interesting responses.
